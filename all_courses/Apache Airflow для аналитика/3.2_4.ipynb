{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59953720",
   "metadata": {},
   "source": [
    "# Apache Airflow для аналитика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65880c6a",
   "metadata": {},
   "source": [
    "# Решение примера на Airflow: практика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce4ec0",
   "metadata": {},
   "source": [
    "## Настройка Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea73afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка Airflow\n",
    "\n",
    "#!pip install apache-airflow==2.1.4\n",
    "#!pip install wtforms==2.3.3\n",
    "\n",
    "# Инициализация базы данных\n",
    "\n",
    "#!airflow db init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b1ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим необходимые папки\n",
    "\n",
    "#!mkdir /root/airflow/dags\n",
    "#!touch /root/airflow/dags/dag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2b3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Включим веб-сервер\n",
    "\n",
    "#!airflow webserver -p 18273 -D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b01d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Последующие команды не имеют отношения к Airflow\n",
    "# Они нужны только для корректной работы веб морды\n",
    "# в среде Google Colab\n",
    "\n",
    "#!pip install pyngrok\n",
    "#!ngrok authtoken <> # найти его можно https://dashboard.ngrok.com/get-started/setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6eae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим пользователя Airflow\n",
    "\n",
    "#!airflow users create \\\n",
    "#          --username admin \\\n",
    "#          --firstname admin \\\n",
    "#          --lastname admin \\\n",
    "#          --role Admin \\\n",
    "#          --email admin@example.org \\\n",
    "#          -p 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999f884",
   "metadata": {},
   "source": [
    "Поместим в dag.py следующий код:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837b557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from airflow import DAG\n",
    "#from datetime import timedelta\n",
    "#from airflow.utils.dates import days_ago\n",
    "#from airflow.operators.dummy_operator import DummyOperator\n",
    "\n",
    "#dag = DAG('dag',schedule_interval=timedelta(days=1), start_date=days_ago(1))\n",
    "#t1 = DummyOperator(task_id='task_1', dag=dag)\n",
    "#t2 = DummyOperator(task_id='task_2',dag=dag)\n",
    "#t3 = DummyOperator(task_id='task_3',dag=dag)\n",
    "#t4 = DummyOperator(task_id='task_4',dag=dag)\n",
    "#t5 = DummyOperator(task_id='task_5',dag=dag)\n",
    "#t6 = DummyOperator(task_id='task_6',dag=dag)\n",
    "#t7 = DummyOperator(task_id='task_7',dag=dag)\n",
    "\n",
    "#[t1, t2]>>t5\n",
    "#t3>>t6\n",
    "#[t5,t6] >>  t7\n",
    "#t4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6106d1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск шедулера\n",
    "\n",
    "#!airflow scheduler -D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8bebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Эта команда просто отображет веб морду на другой адрес\n",
    "# Его вы можете найти https://dashboard.ngrok.com/cloud-edge/endpoints\n",
    "# При каждом отключении ссылка будет меняться\n",
    "\n",
    "#!nohup ngrok http 18273 > /dev/null &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f673dad9",
   "metadata": {},
   "source": [
    "После запуска команды выше, перейдите по адресу в ngrok и подождите пока появится DAG с именем dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b906334",
   "metadata": {},
   "source": [
    "## Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b6a7ee",
   "metadata": {},
   "source": [
    "В прошлой проектной работе вы реализовали ETL скрипт который выгружает данные из сторонних источников. Теперь я предлагаю вам взять небольшую его часть и переписать с помощью Airflow. Использовать только 1 дату 2021-01-01 можно прописать в функции напрямую, захардкодить.\n",
    "\n",
    "Вам необходимо обернуть ваш код в PythonOperator\n",
    "\n",
    "- Скачайте валюту за 2021-01-01 и положите в CSV файл на диске (использовать PythonOperator чтобы скачать данные, можно использовать pandas)\n",
    "\n",
    "- Скачайте логи финансовых транзакций за 2021-01-01 и положите в CSV файл на диске (использовать PythonOperator чтобы скачать данные, можно использовать pandas)\n",
    "\n",
    "- Объединить данные по дате и сложить в таблицу в SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# необходимые импорты\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "import sqlite3\n",
    "\n",
    "# БД, с которой будем работать\n",
    "CON = sqlite3.connect('sqlite3.db')\n",
    "\n",
    "# функции, которые будем использовать\n",
    "def extract_data(url, tmp_file, **context):\n",
    "    pd.read_csv(url).to_csv(tmp_file) # изменение to_csv, запишем данные в файл\n",
    "    \n",
    "def transform_data(group, agreg, tmp_file, tmp_agg_file, **context):\n",
    "    data = pd.read_csv(tmp_file) # изменение read_csv\n",
    "    data.groupby(group).agg(agreg).reset_index().to_csv(tmp_agg_file) # изменение to_csv, запишем данные в файл\n",
    "    \n",
    "def merge_data(data1, data2, tmp_file):\n",
    "    result=data1.merge(data2, on='date')\n",
    "    pd.read_csv(result).to_csv(tmp_file)\n",
    "\n",
    "def load_data(tmp_file, table_name, conn=CON, **context):\n",
    "    data = pd.read_csv(tmp_file)# изменение read_csv, прочитаем данные из файла\n",
    "    data[\"insert_time\"] = pd.to_datetime(\"now\")\n",
    "    data.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    \n",
    "    \n",
    "# Создаем DAG, в который поместим наши задачи\n",
    "# Для DAG-а нужно задать следующие обязательные атрибуты:\n",
    "# - уникальное имя\n",
    "# - интервал запусков\n",
    "# - начальная точка запуска\n",
    "\n",
    "dag = DAG(dag_id='dag', # уникальное имя дага\n",
    "         default_args={'owner': 'airflow'}, # список необязательных аргументов\n",
    "         schedule_interval='@daily', # интервал запусков, в данном случае 1 раз в день 24:00\n",
    "         start_date=days_ago(1) # начальная точка запуска, это с какого момента мы бы хотели, чтобы скрипт начал исполняться\n",
    "    )\n",
    "\n",
    "# задача для скачивания курса валюты за 2021-01-01\n",
    "extract_excangerate = PythonOperator(\n",
    "    task_id='extract_excangerate', # имя задачи внутри Dag\n",
    "    dag=dag,\n",
    "    python_callable=extract_data, # запускаемая Python функция, описана выше\n",
    "    # аргументы функции\n",
    "    op_kwargs={\n",
    "        'url': 'https://raw.githubusercontent.com/datanlnja/airflow_course/main/excangerate/2021-01-01.csv',\n",
    "        'tmp_file': '/tmp/file1.csv'}\n",
    "    )\n",
    "\n",
    "# задача для скачивания логов финансовых транзакций за 2021-01-01\n",
    "extract_data = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    dag=dag,\n",
    "    python_callable=extract_data, \n",
    "    op_kwargs={\n",
    "        'url': 'https://raw.githubusercontent.com/datanlnja/airflow_course/main/data/2021-01-01.csv',\n",
    "        'tmp_file': '/tmp/file2.csv'}\n",
    "    )\n",
    "\n",
    "# задача объединения данных по дате\n",
    "merge_data = PythonOperator(\n",
    "    task_id='merge_data',\n",
    "    dag=dag,\n",
    "    python_callable = merge_data,\n",
    "    op_kwargs={\n",
    "        'data1':'/tmp/file1.csv',\n",
    "        'data2':'/tmp/file2.csv',\n",
    "        'tmp_file': '/tmp/file_merged.csv'}\n",
    "    )\n",
    "\n",
    "# задача загрузки данных в БД\n",
    "load_data = PythonOperator(\n",
    "    task_id='load_data',\n",
    "    dag=dag,\n",
    "    python_callable=load_data,\n",
    "    op_kwargs={\n",
    "        'tmp_file': '/tmp/file_merged.csv',\n",
    "        'table_name': 'table'}\n",
    "    )\n",
    "\n",
    "# порядок выполнения задач\n",
    "extract_excangerate >> extract_data >> merge_data >> load_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3806e43",
   "metadata": {},
   "source": [
    "\n",
    "_____________________________________________________________________________________________________________________________\n",
    "Проектную работу выполнила: аналитик данных Малахова Наталья\n",
    "\n",
    "Мой телеграм-канал: [Дневник аналитика](https://t.me/diary_musician_analyst \"Дневник аналитика\")\n",
    "\n",
    "GitHub: [GitHub](https://github.com/Malakhova-Natalya \"GitHub\")\n",
    "\n",
    "Habr: [Habr](https://habr.com/ru/users/Malakhova-Natalya/publications/articles/ \"Habr\")\n",
    "\n",
    "Спасибо за внимание!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Содержание",
   "title_sidebar": "Содержание",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
